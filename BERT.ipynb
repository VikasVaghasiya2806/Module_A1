{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3bf1be7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (52243, 3) (60%)\n",
      "Validation data shape: (17415, 3) (20%)\n",
      "Test data shape: (17415, 3) (20%)\n",
      "Original test dataset shape (from file): (87074, 3)\n",
      "Target categories: ['Any Other Cyber Crime', 'Cryptocurrency Crime', 'Cyber Attack/ Dependent Crimes', 'Cyber Terrorism', 'Hacking  Damage to computercomputer system etc', 'Online Cyber Trafficking', 'Online Financial Fraud', 'Online Gambling  Betting', 'Online and Social Media Related Crime', 'Ransomware']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "58"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(1)\n",
    "\n",
    "import regex as re\n",
    "from sklearn import metrics\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shutil\n",
    "import sys\n",
    "from pathlib import Path\n",
    "# from time import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datetime import datetime, date\n",
    "TODAY_DATETIME = str(datetime.now())[:16].replace('-','').replace(' ','_').replace(':','')\n",
    "\n",
    "# from transformers import BertTokenizer, BertModel\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define constants and paths\n",
    "USER_FOLDER = \"/home/202462003\"\n",
    "TODAY_DATETIME = str(datetime.now())[:16].replace('-', '').replace(' ', '_').replace(':', '')\n",
    "CKPT_PATH = f\"{USER_FOLDER}/checkpoints/distilbert-base/AllLayers/{TODAY_DATETIME}\"\n",
    "BEST_MODEL_PATH = f\"{CKPT_PATH}/best_model.pt\"\n",
    "\n",
    "# Dynamic user directory path setting\n",
    "user = os.path.dirname(os.path.realpath(sys.argv[0])).split('/')[2]\n",
    "processed_dir_path = f\"/home/{user}\"\n",
    "training_dataset_pathname = f\"{processed_dir_path}/train.csv\"\n",
    "validation_dataset_pathname = f\"{processed_dir_path}/test.csv\"\n",
    "\n",
    "# Load the datasets\n",
    "train_df = pd.read_csv(training_dataset_pathname)\n",
    "test_df = pd.read_csv(validation_dataset_pathname)\n",
    "\n",
    "# Drop rows with NaN in the 'category' column\n",
    "train_df = train_df.dropna(subset=['category'])\n",
    "train_df = train_df.dropna(subset=['sub_category'])\n",
    "train_df = train_df.dropna(subset=['crimeaditionalinfo'])\n",
    "\n",
    "# Drop rows with NaN in the 'category' column\n",
    "test_df = train_df.dropna(subset=['category'])\n",
    "test_df = train_df.dropna(subset=['sub_category'])\n",
    "test_df = train_df.dropna(subset=['crimeaditionalinfo'])\n",
    "\n",
    "\n",
    "# Drop rows with NaN in the 'category' column\n",
    "train_df = train_df.dropna(subset=['category'])\n",
    "\n",
    "# Filter out classes with fewer than 2 samples to allow stratification\n",
    "train_df = train_df.groupby('category').filter(lambda x: len(x) > 1)\n",
    "\n",
    "# First split: 60% for training, 40% remaining\n",
    "train_data, temp_data = train_test_split(\n",
    "    train_df, test_size=0.4, random_state=42, stratify=train_df['category']\n",
    ")\n",
    "\n",
    "# Second split: split remaining 40% into 20% validation and 20% test\n",
    "val_data, test_data = train_test_split(\n",
    "    temp_data, test_size=0.5, random_state=42, stratify=temp_data['category']\n",
    ")\n",
    "\n",
    "# Print the sizes of each dataset\n",
    "print(f\"Training data shape: {train_data.shape} (60%)\")\n",
    "print(f\"Validation data shape: {val_data.shape} (20%)\")\n",
    "print(f\"Test data shape: {test_data.shape} (20%)\")\n",
    "print(f\"Original test dataset shape (from file): {test_df.shape}\")\n",
    "\n",
    "# Get sorted list of unique target categories\n",
    "target_list = sorted(train_df['category'].unique())\n",
    "print(\"Target categories:\", target_list)\n",
    "\n",
    "# Cleanup\n",
    "del train_df, temp_data\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b547502f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e9120a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f01531",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7a91a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, df, tokenizer, max_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.df = df\n",
    "        self.title = df['crimeaditionalinfo']\n",
    "        self.targets = self.df[target_list].values\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.title)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        title = str(self.title[index])\n",
    "        title = \" \".join(title.split())\n",
    "\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            title,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            return_token_type_ids=False,\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': inputs['input_ids'].flatten(),\n",
    "            'attention_mask': inputs['attention_mask'].flatten(),\n",
    "            'targets': torch.FloatTensor(self.targets[index])\n",
    "        }\n",
    "\n",
    "def loss_fn(outputs, targets):\n",
    "    return torch.nn.BCEWithLogitsLoss()(outputs, targets)\n",
    "\n",
    "def maxindex(data_list):\n",
    "    index=0\n",
    "    for i in range(len(data_list)):\n",
    "        if (data_list[i]>data_list[index]):\n",
    "            index=i\n",
    "    return index\n",
    "\n",
    "def accuracy(predictions, targets):\n",
    "    batch_size=len(predictions)\n",
    "    count=0.0\n",
    "    for i in range(batch_size):\n",
    "        if maxindex(predictions[i]) == maxindex(targets[i]):\n",
    "            count=count+1\n",
    "            # print(count)\n",
    "    # print(f\"predictions: {predictions[0]}\")\n",
    "    # print(f\"targets: {targets[0]}\")\n",
    "    \n",
    "    return count*(100.0/batch_size)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def load_ckp(checkpoint_fpath, model, optimizer):\n",
    "    \"\"\"\n",
    "    checkpoint_path: path to save checkpoint\n",
    "    model: model that we want to load checkpoint parameters into       \n",
    "    optimizer: optimizer we defined in previous training\n",
    "    \"\"\"\n",
    "    # load check point\n",
    "    checkpoint = torch.load(checkpoint_fpath)\n",
    "    # initialize state_dict from checkpoint to model\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    # initialize optimizer from checkpoint to optimizer\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    # initialize valid_loss_min from checkpoint to valid_loss_min\n",
    "    valid_loss_min = checkpoint['valid_loss_min']\n",
    "    # return model, optimizer, epoch value, min validation loss \n",
    "    return model, optimizer, checkpoint['epoch'], valid_loss_min.item()\n",
    "\n",
    "def save_ckp(state, is_best, checkpoint_path, best_model_path, epoch):\n",
    "    \"\"\"\n",
    "    state: checkpoint we want to save\n",
    "    is_best: is this the best checkpoint; min validation loss\n",
    "    checkpoint_path: path to save checkpoint\n",
    "    best_model_path: path to save best model\n",
    "    \"\"\"\n",
    "    Path(checkpoint_path).mkdir(parents=True, exist_ok=True)\n",
    "    f_path = checkpoint_path + '/ckpt_epoch' + str(epoch) + '.pt'    \n",
    "    # save checkpoint data to the path given, checkpoint_path\n",
    "    torch.save(state, f_path)\n",
    "    # if it is a best model, min validation loss\n",
    "    if is_best:\n",
    "        best_fpath = best_model_path\n",
    "        # copy that checkpoint file to best path given, best_model_path\n",
    "        shutil.copyfile(f_path, best_fpath)\n",
    "\n",
    "\n",
    "class SequenceClassifier(torch.nn.Module):\n",
    "    def __init__(self,num_labels):\n",
    "        super(SequenceClassifier, self).__init__()\n",
    "        self.classifier_model = AutoModelForSequenceClassification.from_pretrained(\"distilbert/distilbert-base-uncased\", num_labels = num_labels)\n",
    "    \n",
    "    def forward(self, input_ids, attn_mask):\n",
    "        output = self.classifier_model(\n",
    "            input_ids,\n",
    "            attention_mask=attn_mask\n",
    "        )\n",
    "        # print('output Tensor:')\n",
    "        # print(f\"{output}\")\n",
    "        return output\n",
    "\n",
    "\n",
    "\n",
    "def train_model(n_epochs, training_loader, validation_loader, model, \n",
    "                optimizer, checkpoint_path, best_model_path): \n",
    "   \n",
    "    # initialize tracker for minimum validation loss\n",
    "    valid_loss_min = np.Inf\n",
    "\n",
    "\n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        train_loss = 0\n",
    "        valid_loss = 0\n",
    "        total_accuracy = 0.0\n",
    "        total_validation_accuracy = 0.0\n",
    "\n",
    "        model.train()\n",
    "        pbar = tqdm(total=num_train_batches,colour='green')\n",
    "        lossbar = tqdm(total=1.0)\n",
    "        bw_train_loss = []\n",
    "        print('############# Epoch {}: Training Start   #############'.format(epoch))\n",
    "        for batch_idx, data in enumerate(training_loader):\n",
    "            #print('yyy epoch', batch_idx)\n",
    "            pbar.update(1)\n",
    "            ids = data['input_ids'].to(device, dtype = torch.long)\n",
    "            mask = data['attention_mask'].to(device, dtype = torch.long)\n",
    "            targets = data['targets'].to(device, dtype = torch.float)\n",
    "            \n",
    "            # print(f\"targets:\\n{targets}\")\n",
    "\n",
    "            outputs = model(ids, mask).logits\n",
    "            \n",
    "            # print(f\"outputs:\\n{outputs}\")\n",
    "\n",
    "            # _, predicted = torch.max(outputs.data, 1)\n",
    "            \n",
    "            batch_accuracy = accuracy(outputs, targets)\n",
    "            total_accuracy += batch_accuracy\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            # print(f'Epoch: {epoch}, Training Loss:  {loss.item()}')\n",
    "            # if batch_idx%5000==0:\n",
    "             #   print(f'Epoch: {epoch}, Training Loss:  {loss.item()}')\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # print('before loss data in training', loss.item(), train_loss)\n",
    "            bw_train_loss.append(loss)\n",
    "            # print('after loss data in training', loss.item(), train_loss)\n",
    "\n",
    "            lossbar.n= loss.item()\n",
    "            lossbar.refresh()\n",
    "\n",
    "        print('############# Epoch {}: Training End     #############'.format(epoch))\n",
    "        training_accuracy = total_accuracy / len(training_loader)\n",
    "        print(f\"training accuracy is {training_accuracy}\")\n",
    "        pbar.close()\n",
    "        lossbar.close()\n",
    "\n",
    "        print('############# Epoch {}: Validation Start   #############'.format(epoch))\n",
    "        ######################  \n",
    "        # validate the model #\n",
    "        #####################\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pbar = tqdm(total=num_val_batches, colour='green')\n",
    "            lossbar = tqdm(total=1.0)\n",
    "            bw_val_loss = []\n",
    "            for batch_idx, data in enumerate(validation_loader):\n",
    "                pbar.update(1)\n",
    "                ids = data['input_ids'].to(device, dtype = torch.long)\n",
    "                mask = data['attention_mask'].to(device, dtype = torch.long)\n",
    "                targets = data['targets'].to(device, dtype = torch.float)\n",
    "                outputs = model(ids, mask).logits\n",
    "                \n",
    "                #_, predicted = torch.max(outputs.data, 1)\n",
    "                batch_accuracy = accuracy(outputs, targets)\n",
    "                total_validation_accuracy += batch_accuracy\n",
    "\n",
    "                loss = loss_fn(outputs, targets)\n",
    "                bw_val_loss.append(loss)\n",
    "                \n",
    "                val_targets.extend(targets.cpu().detach().numpy().tolist())\n",
    "                val_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n",
    "\n",
    "                lossbar.n= loss.item()\n",
    "                lossbar.refresh()\n",
    "\n",
    "            print('############# Epoch {}: Validation End     #############'.format(epoch))\n",
    "            pbar.close()\n",
    "            lossbar.close()\n",
    "            # calculate average losses\n",
    "            #print('before cal avg train loss', train_loss)\n",
    "            train_loss = sum(bw_train_loss)/len(training_loader)\n",
    "            valid_loss = sum(bw_val_loss)/len(validation_loader)\n",
    "            # print training/validation statistics \n",
    "            print('Epoch: {} \\tAvgerage Training Loss: {:.6f} \\tAverage Validation Loss: {:.6f}'.format(\n",
    "                    epoch, \n",
    "                    train_loss,\n",
    "                    valid_loss\n",
    "                    ))\n",
    "            \n",
    "            validation_accuracy = total_validation_accuracy / len(validation_loader)\n",
    "            print(f\"validation accuracy is {validation_accuracy}\")\n",
    "            \n",
    "            # create checkpoint variable and add important data\n",
    "            checkpoint = {\n",
    "                    'epoch': epoch + 1,\n",
    "                    'valid_loss_min': valid_loss,\n",
    "                    'state_dict': model.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict()\n",
    "            }\n",
    "            \n",
    "            # save checkpoint\n",
    "            save_ckp(checkpoint, False, checkpoint_path, best_model_path, epoch)\n",
    "        \n",
    "            ## TODO: save the model if validation loss has decreased\n",
    "            if valid_loss <= valid_loss_min:\n",
    "                print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,valid_loss))\n",
    "                # save checkpoint as best model\n",
    "                save_ckp(checkpoint, True, checkpoint_path, best_model_path, epoch)\n",
    "                valid_loss_min = valid_loss\n",
    "\n",
    "        print('############# Epoch {}  Done   #############\\n'.format(epoch))\n",
    "\n",
    "    return model\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    df_train = pd.read_csv(f\"{training_dataset_pathname}\", index_col=False)\n",
    "    df_val = pd.read_csv(f\"{validation_dataset_pathname}\", index_col=False)\n",
    "    \n",
    "    df_train_v1 = df_train.loc[:, ['crimeaditionalinfo', 'category']]\n",
    "    df_train_v1 = df_train_v1.reset_index(drop=True)\n",
    "\n",
    "    df_val_v1 = df_val.loc[:, ['crimeaditionalinfo', 'category']]\n",
    "    df_val_v1 = df_val_v1.reset_index(drop=True)\n",
    "\n",
    "    case_tags_train = pd.get_dummies(df_train_v1['category'], dtype=int)\n",
    "    case_tags_train_columns = sorted(case_tags_train.columns.tolist())\n",
    "    case_tags_train=case_tags_train[case_tags_train_columns]\n",
    "\n",
    "    df_train_v2 = pd.concat([df_train_v1, case_tags_train], axis=1)\n",
    "\n",
    "\n",
    "    case_tags_val = pd.get_dummies(df_val_v1['category'], dtype=int)\n",
    "    case_tags_val_columns = case_tags_val.columns.tolist()\n",
    "\n",
    "    # to add missing columns in validation dataframe \n",
    "    for column in case_tags_train_columns:\n",
    "        if column not in case_tags_val_columns:\n",
    "            case_tags_val[column]=pd.Series(0,index=range(case_tags_val.shape[0]))\n",
    "    case_tags_val_columns = sorted(case_tags_val.columns.tolist())\n",
    "    case_tags_val=case_tags_val[case_tags_val_columns]\n",
    "\n",
    "    df_val_v2 = pd.concat([df_val_v1, case_tags_val], axis=1)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c135ebe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "    MAX_LEN = 256\n",
    "    # TRAIN_BATCH_SIZE = 8\n",
    "    # VALID_BATCH_SIZE = 8\n",
    "    TRAIN_BATCH_SIZE = 64\n",
    "    VALID_BATCH_SIZE = 64\n",
    "    EPOCHS = 15\n",
    "    LEARNING_RATE = 1e-05\n",
    "\n",
    "    # tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")\n",
    "    \n",
    "    train_dataset = CustomDataset(df_train_v2, tokenizer, MAX_LEN)\n",
    "    valid_dataset = CustomDataset(df_val_v2, tokenizer, MAX_LEN)\n",
    "\n",
    "    train_data_loader = torch.utils.data.DataLoader(train_dataset, \n",
    "        batch_size=TRAIN_BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        num_workers=0\n",
    "    )\n",
    "\n",
    "    num_train_batches = len(train_data_loader)\n",
    "\n",
    "    val_data_loader = torch.utils.data.DataLoader(valid_dataset, \n",
    "        batch_size=VALID_BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        num_workers=0\n",
    "    )\n",
    "\n",
    "    num_val_batches = len(val_data_loader)\n",
    "\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    # device = torch.device('cpu')\n",
    "    \n",
    "    print(f\"target list is {target_list}\")\n",
    "\n",
    "    model = SequenceClassifier(len(target_list))\n",
    "    model.to(device)\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(params =  model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "\n",
    "    val_targets=[]\n",
    "    val_outputs=[]\n",
    "    \n",
    "    Path(CKPT_PATH).mkdir(parents=True, exist_ok=True)\n",
    "    trained_model = train_model(EPOCHS, train_data_loader, val_data_loader, model, optimizer, CKPT_PATH, BEST_MODEL_PATH)\n",
    "    print(f\"saved model to {BEST_MODEL_PATH}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3878fe06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f00225",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ffbbe0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13754665",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4759c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
